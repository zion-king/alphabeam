{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "# import openai\n",
    "import tiktoken \n",
    "import cohere\n",
    "import chromadb\n",
    "import tempfile\n",
    "import google.generativeai as genai\n",
    "from llama_index.llms import OpenAI, Gemini\n",
    "from llama_index.memory import ChatMemoryBuffer\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, PromptHelper, LLMPredictor, load_index_from_storage\n",
    "from llama_index.embeddings import OpenAIEmbedding, GeminiEmbedding\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.vector_stores.google.generativeai import GoogleVectorStore, set_google_config, genai_extension as genaix\n",
    "from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer, LLMRerank, CohereRerank, LongContextReorder\n",
    "from llama_index import download_loader\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from flask import Flask, jsonify, flash, request, redirect, render_template, url_for, Response, stream_with_context\n",
    "from google.oauth2 import service_account\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = ''\n",
    "os.environ[\"GOOGLE_API_KEY\"]  = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    client_options={\"api_endpoint\": \"generativelanguage.googleapis.com\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_google_config(api_endpoint=GOOGLE_API_KEY)\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    \"../backend/credentials.json\",\n",
    "    scopes=[\n",
    "        \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "        \"https://www.googleapis.com/auth/generative-language.retriever\",\n",
    "    ],\n",
    ")\n",
    "set_google_config(auth_credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMADB_HOST = \"localhost\"\n",
    "COHERE_RERANK_KEY = ''\n",
    "ALLOWED_EXTENSIONS = {'txt', 'htm', 'html', 'pdf', 'doc', 'docx', 'ppt', 'pptx', 'csv'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the chroma server, run the following in terminal:\n",
    "\n",
    "`chroma run --path ./src/vector_db`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Reader for `.yml` and `.sql` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.schema import Document\n",
    "\n",
    "\n",
    "class YMLReader(BaseReader):\n",
    "    def load_data(self, file, extra_info=None):\n",
    "        with open(file, \"r\") as f:\n",
    "            print(file)\n",
    "            text = f.read()\n",
    "        # load_data returns a list of Document objects\n",
    "        return [Document(text=text + \"Foobar\", extra_info={\"filename\": str(file), \"file_type\": \".yml\"})]\n",
    "        \n",
    "class SQLReader(BaseReader):\n",
    "    def load_data(self, file, extra_info=None):\n",
    "        with open(file, \"r\") as f:\n",
    "            print(file)\n",
    "            text = f.read()\n",
    "        # load_data returns a list of Document objects\n",
    "        return [Document(text=text + \"Foobar\", extra_info={\"filename\": str(file), \"file_type\": \".sql\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_embedding_chroma(index_name, temp_dir):\n",
    "        \n",
    "    try:\n",
    "        # initialize client, setting path to save data\n",
    "        # db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        print(\"Connecting to Chroma database...\")\n",
    "        db = chromadb.HttpClient(host=CHROMADB_HOST, port=8000)\n",
    "    except:\n",
    "        return {'statusCode': 400, 'status': 'Could not connect to chroma database'}\n",
    "\n",
    "    try:\n",
    "        # create collection\n",
    "        print(\"Creating vector embeddings......\")\n",
    "        print(\"Index name: \", index_name)\n",
    "        start_time = time.time()\n",
    "        chroma_collection = db.get_or_create_collection(\n",
    "            name=index_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"} # default: L2; used before: ip\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\"Error : : :\", e)\n",
    "        return {'statusCode': 400, 'status': 'A knowledge base with the same name already exists'}\n",
    "\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    # setup our storage (vector db)\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "\n",
    "    llm = Gemini(api_key=GOOGLE_API_KEY, model='models/gemini-pro', temperature=0.6)\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    node_parser = SimpleNodeParser.from_defaults(\n",
    "        # text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "        )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "        )\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_dir=temp_dir,\n",
    "                                      file_extractor={\".yml\": YMLReader(), \".sql\": SQLReader()} # extra custom extractor\n",
    "                                    ).load_data()\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    # temp_dir.cleanup() # delete document temp dir\n",
    "\n",
    "    print(f\"Vector embeddings created in {time.time() - start_time} seconds.\")\n",
    "\n",
    "    response = {\n",
    "        'statusCode': 200,\n",
    "        'status': 'Chroma embedding complete',\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_embedding_google(index_name, temp_dir):\n",
    "\n",
    "    start_time = time.time()\n",
    "    vector_store = GoogleVectorStore.create_corpus(corpus_id=index_name) # param:display_name\n",
    "\n",
    "    llm = Gemini(model='models/gemini-pro', temperature=0)\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    node_parser = SimpleNodeParser.from_defaults(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "        )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser,\n",
    "        )\n",
    "\n",
    "    # documents = SimpleDirectoryReader(input_dir=temp_dir,\n",
    "    #                                   file_extractor={\".yml\": YMLReader(), \".sql\": SQLReader()} # extra custom extractor\n",
    "    #                                 ).load_data()\n",
    "    \n",
    "    documents = SimpleDirectoryReader(input_dir=temp_dir).load_data()\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        vector_store=vector_store,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    # temp_dir.cleanup() # delete document temp dir\n",
    "\n",
    "    print(f\"Vector embeddings created in {time.time() - start_time} seconds.\")\n",
    "\n",
    "    response = {\n",
    "        'statusCode': 200,\n",
    "        'status': 'Chroma embedding complete',\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_vector_db(index_name):\n",
    "    \n",
    "    try:\n",
    "        # initialize client\n",
    "        db = chromadb.HttpClient(host=CHROMADB_HOST, port=8000)\n",
    "    except Exception as e:\n",
    "        print('<<< get_index_from_vector_db() >>> Could not connect to database!\\n', e)\n",
    "        return None, None\n",
    "    \n",
    "    # get collection and embedding size\n",
    "    try:\n",
    "        chroma_collection = db.get_collection(index_name)\n",
    "        doc_size = chroma_collection.count()\n",
    "        print('Computing knowledge base size...', doc_size)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "    start_time = time.time()\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Experimented settings for large docs versus small. Don't change except you have tested extensively!!!\n",
    "    # context_window=65000 if doc_size>200 else 16384\n",
    "    context_window=32000 if doc_size>300 else 16000\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    llm = Gemini(api_key=GOOGLE_API_KEY, model='models/gemini-pro', temperature=0)\n",
    "    print_msg = \"Using Gemini Pro...\"\n",
    "    print(print_msg)\n",
    "\n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "    #     )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        context_window=context_window, \n",
    "        embed_model=embed_model,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    print('Retrieving knowledge base index from ChromaDB...')\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store, \n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    print(f'Index retrieved from ChromaDB in {time.time() - start_time} seconds.')\n",
    "    return index, doc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_vector_db_google(index_name):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    vector_store = GoogleVectorStore.from_corpus(corpus_id=index_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # doc_size = len(list(genaix.get_document(corpus_id=index_name, client=credentials)))\n",
    "    # print(doc_size)\n",
    "\n",
    "    # context_window=32000 if doc_size>300 else 16000\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    llm = Gemini(api_key=GOOGLE_API_KEY, model='models/gemini-pro', temperature=0)\n",
    "    print_msg = \"Using Gemini Pro...\"\n",
    "    print(print_msg)\n",
    " \n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "    #     )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        # context_window=context_window, \n",
    "        embed_model=embed_model,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    print('Retrieving knowledge base index from Google vector store...')\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store, \n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    doc_size = len(list(index.index_struct.nodes_dict.values()))\n",
    "    print(doc_size)\n",
    "\n",
    "    print(f'Index retrieved from ChromaDB in {time.time() - start_time} seconds.')\n",
    "    return index, doc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_choice_select_answer_fn(\n",
    "    answer: str, num_choices: int, raise_error: bool = False\n",
    "):\n",
    "    \"\"\"Default parse choice select answer function.\"\"\"\n",
    "    answer_lines = answer.split(\"\\n\")\n",
    "    # print(answer_lines)\n",
    "    answer_nums = []\n",
    "    answer_relevances = []\n",
    "    for answer_line in answer_lines:\n",
    "        line_tokens = answer_line.split(\",\")\n",
    "        if len(line_tokens) != 2:\n",
    "            if not raise_error:\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid answer line: {answer_line}. \"\n",
    "                    \"Answer line must be of the form: \"\n",
    "                    \"answer_num: <int>, answer_relevance: <float>\"\n",
    "                )\n",
    "        if len(line_tokens[0].split(\":\"))>1 and line_tokens[0].split(\":\")[1].strip().isdigit():\n",
    "            answer_num = int(line_tokens[0].split(\":\")[1].strip())\n",
    "            if answer_num > num_choices:\n",
    "                continue\n",
    "            answer_nums.append(answer_num)\n",
    "            answer_relevances.append(float(line_tokens[1].split(\":\")[1].strip()))\n",
    "    # print(answer_nums)\n",
    "    return answer_nums, answer_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessor_args(doc_size):\n",
    "    if doc_size<30:\n",
    "        return None\n",
    "    \n",
    "    print('Optimising context information...')\n",
    "    \n",
    "    # fastest postprocessor\n",
    "    cohere_rerank = CohereRerank(api_key=COHERE_RERANK_KEY, top_n=30)\n",
    "\n",
    "    # slower postprocessor\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model, chunk_size=256, chunk_overlap=20) # use llama_index default MockLLM (faster)\n",
    "\n",
    "    rank_postprocessor = LLMRerank(\n",
    "        choice_batch_size=10, top_n=100,\n",
    "        service_context=service_context,\n",
    "        parse_choice_select_answer_fn=parse_choice_select_answer_fn\n",
    "    ) \\\n",
    "        if doc_size>100 \\\n",
    "            else None\n",
    "    \n",
    "    # node postprocessors run in the specified order\n",
    "    node_postprocessors = [\n",
    "        rank_postprocessor,\n",
    "        cohere_rerank,\n",
    "    ] \\\n",
    "        if doc_size>100 \\\n",
    "            else [cohere_rank]\n",
    "\n",
    "    return node_postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_stream(query, index_name, chat_history, prompt_style):\n",
    "\n",
    "    index, doc_size = get_index_from_vector_db(index_name)\n",
    "    # prompt_header = prompt_style()\n",
    "\n",
    "    if index is None:\n",
    "        response = \"Requested information not found\"\n",
    "        return response\n",
    "    else:\n",
    "        node_postprocessors = postprocessor_args(doc_size)\n",
    "        # similarity_top_k = 50 if doc_size>500 else 10\n",
    "        similarity_top_k = 200 if doc_size>200 else doc_size\n",
    "        chat_engine = index.as_chat_engine(chat_mode=\"context\", \n",
    "                                            memory=chat_history,\n",
    "                                            system_prompt=prompt_style, \n",
    "                                            similarity_top_k=similarity_top_k,\n",
    "                                            verbose=True, \n",
    "                                            # streaming=True,\n",
    "                                            function_call=\"query_engine_tool\",\n",
    "                                            node_postprocessors=node_postprocessors\n",
    "                                            )\n",
    "\n",
    "        message_body = f\"\"\"\\nUse the tool to answer:\\n{query}\\n\"\"\"\n",
    "        response = chat_engine.chat(message_body)\n",
    "        # print(get_formatted_sources(response) if response.source_nodes else None)\n",
    "        \n",
    "        if response is None:\n",
    "            print(\"Index retrieved but cannot stream response...\")\n",
    "            chat_response = \"I'm sorry I couldn't find an answer to the requested information in your knowledge base. Please rephrase your question and try again.\"\n",
    "            # for token in chat_response.split():\n",
    "            #     print(token, end=\" \")\n",
    "            #     yield f\"\"\"{token} \"\"\"\n",
    "            return chat_response\n",
    "        else:\n",
    "            print('Starting response stream...\\n...........................\\n...........................')\n",
    "            # return response.response\n",
    "            # for token in response.response_gen:\n",
    "            #     print(token, end=\"\")\n",
    "            #     yield f\"\"\"{token}\"\"\"\n",
    "            return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_formatted_sources(response, length=100, trim_text=True) -> str:\n",
    "    \"\"\"Get formatted sources text.\"\"\"\n",
    "    from llama_index.utils import truncate_text\n",
    "    texts = []\n",
    "    for source_node in response.source_nodes:\n",
    "        fmt_text_chunk = source_node.node.get_content()\n",
    "        if trim_text:\n",
    "            fmt_text_chunk = truncate_text(fmt_text_chunk, length)\n",
    "        # node_id = source_node.node.node_id or \"None\"\n",
    "        node_id = source_node.node.metadata['page_label'] or \"None\"\n",
    "        source_text = f\"> Source (Page no: {node_id}): {fmt_text_chunk}\"\n",
    "        texts.append(source_text)\n",
    "    return \"\\n\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_prompt_style(): \n",
    "\n",
    "    prompt_header = f\"\"\"Your name is Alpha, a highly intelligent system for conversational business intelligence.\n",
    "    Your task is to use the provided knowledege base, containing semantic models of a business dataset,\n",
    "    to determine if my question can be answered based on the semantic knowledge. \n",
    "    If the semantic knowledge contains the parameter(s) useful for answering my question, respond with a Yes, and No otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt_header   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gen_prompt_style(): \n",
    "\n",
    "    with open(\"./assistants/mf_few_shot.txt\", \"r\") as f:\n",
    "        # print(file)\n",
    "        few_shot_examples = f.read()\n",
    "\n",
    "    prompt_header = f\"\"\"Your name is Alpha, a highly intelligent system for \n",
    "    conversational business intelligence. As an SQL expert, your goal is to use the provided knowledege base, \n",
    "    containing metrics and semantic models of a business dataset, to generate a MetricFlow query command needed to answer the question.\n",
    "    The general syntax for the MetricFlow query command is:\n",
    "    `query --metrics <measure(s)> --group-by <table-1__dimension-1, table-1__dimension-2,..., table-n__dimension-n> --limit <int> --order <table__dimension> --where <condition>`\n",
    "    \n",
    "    To generate the correct MetricFlow query command, determine the following:\n",
    "    1. Which metrics are needed to answer the question?\n",
    "    2. Which tables contain the required data?\n",
    "    3. Which dimensions in the tables contain the required data?\n",
    "    4. Do the results need to be filtered by a specific condition?\n",
    "    5. Do the results need to be ordered by a specific dimension?\n",
    "    6. Is there a limit on the number of records requested?\n",
    "\n",
    "    Here's some examples of how a MetricFlow query command is generated using the information in the knowledge base.\n",
    "    {few_shot_examples}\n",
    "\n",
    "    The measures, dimensions, tables and other parameters referenced in the above examples are \n",
    "    obtained from the knowledege base provided below. Following these examples, generate a single-line query command that answers the question. \n",
    "    Return only this command or return \"Null\" if the provided information is not sufficient to answer the question.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt_header   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returned_data_prompt_style():\n",
    "\n",
    "    prompt_header = f\"\"\"Your name is Alpha, a highly intelligent system for \n",
    "    conversational business intelligence. Your task is to use the entirety of the \n",
    "    provided data, which has been fetched from a database, to answer my question.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt_header   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_run_query(llm_query_input: str):\n",
    "    output = subprocess.run(\"mf \"+ str(llm_query_input), \n",
    "                            cwd=\"./semantics/\",\n",
    "                            # shell=True,\n",
    "                            # text=True,\n",
    "                            capture_output=True,)\n",
    "    print(output.stdout)\n",
    "    return output.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(user_query, llm_query_input):\n",
    "    from llama_index.schema import Document\n",
    "\n",
    "    data = llm_run_query(llm_query_input)\n",
    "    doc = Document(text=str(data))\n",
    "    # doc.doc_id = \"llm-output\"\n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     chunk_size=1024,\n",
    "    #     chunk_overlap=20\n",
    "    #     )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=Gemini(model='models/gemini-pro', temperature=0),\n",
    "        embed_model=GeminiEmbedding(),\n",
    "        # node_parser=node_parser,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "        )\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(doc, service_context=service_context)\n",
    "\n",
    "    chat_engine = index.as_chat_engine(chat_mode=\"context\", \n",
    "                                        memory=chat_history,\n",
    "                                        system_prompt=returned_data_prompt_style(), \n",
    "                                        similarity_top_k=30,\n",
    "                                        verbose=True, \n",
    "                                        # streaming=True,\n",
    "                                        function_call=\"query_engine_tool\",\n",
    "                                        )\n",
    "\n",
    "    message_body = f\"\"\"\\nUse the tool to answer:\\n{user_query}\\n\"\"\"\n",
    "    response = chat_engine.chat(message_body)\n",
    "    \n",
    "    if response is None:\n",
    "        print(\"Index retrieved but couldn't stream response...\")\n",
    "        chat_response = \"I'm sorry I couldn't find an answer to the requested information in your knowledge base. Please rephrase your question and try again.\"\n",
    "        # for token in chat_response.split():\n",
    "        #     print(token, end=\" \")\n",
    "        #     yield f\"\"\"{token} \"\"\"\n",
    "        return chat_response\n",
    "    else:\n",
    "        print('Starting response stream...\\n...........................\\n...........................')\n",
    "        return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic layer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"semantic-layer-6\"\n",
    "index_name = project_name.lower() #+ '_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  semantic-layer-6\n",
      "semantics\\models\\semantic_models\\customers.yml\n",
      "semantics\\models\\semantic_models\\metricflow_time_spine.sql\n",
      "semantics\\models\\semantic_models\\orders.yml\n",
      "semantics\\models\\semantic_models\\products.yml\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\customers.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"330aee08-4639-47f2-9690-e07c190f2e36\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"f17a0c27-60a2-4e7c-bc67-0e34c5b96ad8\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"hash\": \"c277a6f8f700e10e4ddaa1e1b8d1006815a3e7cab07f95921bcc785e9a505384\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"5f9e154b-c74b-4ab4-a18b-c14d50d95c71\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"6e7553c9d005d1e21c4acaf6158bc0cdef0bec395d3db52085b97bc68c0e62ae\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 654, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'f17a0c27-60a2-4e7c-bc67-0e34c5b96ad8')\n",
      "('doc_id', 'f17a0c27-60a2-4e7c-bc67-0e34c5b96ad8')\n",
      "('ref_doc_id', 'f17a0c27-60a2-4e7c-bc67-0e34c5b96ad8')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\metricflow_time_spine.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"5f9e154b-c74b-4ab4-a18b-c14d50d95c71\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"fa5de80a-ad86-4ff6-aff4-f361c1c36de7\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"hash\": \"5ab69159a18e468f0159c267f179366bdf5d1cf5957532107a0e93e9b1a36e0c\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"330aee08-4639-47f2-9690-e07c190f2e36\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"hash\": \"6e7553c9d005d1e21c4acaf6158bc0cdef0bec395d3db52085b97bc68c0e62ae\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"dde25386-fe33-4a64-b706-1dd7a1a66857\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 262, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'fa5de80a-ad86-4ff6-aff4-f361c1c36de7')\n",
      "('doc_id', 'fa5de80a-ad86-4ff6-aff4-f361c1c36de7')\n",
      "('ref_doc_id', 'fa5de80a-ad86-4ff6-aff4-f361c1c36de7')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\orders.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"dde25386-fe33-4a64-b706-1dd7a1a66857\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"33863e50-f66a-434b-b5a1-b870f2e5790b\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"hash\": \"c267a8292f6eedc940c949df963bd1045086dd47275392af78aa08adb86e3748\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"5f9e154b-c74b-4ab4-a18b-c14d50d95c71\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"c7725c1a-553f-4c0e-bdaa-0766fe23fa82\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"c8cecc2bf7d8d1568c416b437fd4b2f2c59d1e4f39bb2209c0a746f5efc9f1d8\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 1778, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '33863e50-f66a-434b-b5a1-b870f2e5790b')\n",
      "('doc_id', '33863e50-f66a-434b-b5a1-b870f2e5790b')\n",
      "('ref_doc_id', '33863e50-f66a-434b-b5a1-b870f2e5790b')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\products.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"c7725c1a-553f-4c0e-bdaa-0766fe23fa82\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\products.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"347f5dda-c3bf-454d-a1d4-4979e260edd2\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\products.yml\", \"file_type\": \".yml\"}, \"hash\": \"8ea46e1942baf1a3a56cc434194b57174ff402caeca30125960dfa203306df61\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"dde25386-fe33-4a64-b706-1dd7a1a66857\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"c8cecc2bf7d8d1568c416b437fd4b2f2c59d1e4f39bb2209c0a746f5efc9f1d8\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 675, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '347f5dda-c3bf-454d-a1d4-4979e260edd2')\n",
      "('doc_id', '347f5dda-c3bf-454d-a1d4-4979e260edd2')\n",
      "('ref_doc_id', '347f5dda-c3bf-454d-a1d4-4979e260edd2')\n",
      "Vector embeddings created in 4.27899956703186 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'status': 'Chroma embedding complete'}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vector_embedding(index_name, './semantics/models/semantic_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Project has the maximum number of Corpora (5).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:79\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\grpc\\_channel.py:1160\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1154\u001b[0m (\n\u001b[0;32m   1155\u001b[0m     state,\n\u001b[0;32m   1156\u001b[0m     call,\n\u001b[0;32m   1157\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(\n\u001b[0;32m   1158\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1159\u001b[0m )\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\grpc\\_channel.py:1003\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1003\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Project has the maximum number of Corpora (5).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.184.170:443 {created_time:\"2023-12-22T09:12:17.8752899+00:00\", grpc_status:8, grpc_message:\"Project has the maximum number of Corpora (5).\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Zion\\alphabeam\\src\\semantic_retrieval.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_vector_embedding_google(index_name, \u001b[39m'\u001b[39;49m\u001b[39m./semantics/models/semantic_models/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\HP\\Zion\\alphabeam\\src\\semantic_retrieval.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_vector_embedding_google\u001b[39m(index_name, temp_dir):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vector_store \u001b[39m=\u001b[39m GoogleVectorStore\u001b[39m.\u001b[39;49mcreate_corpus(corpus_id\u001b[39m=\u001b[39;49mindex_name) \u001b[39m# param:display_name\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     llm \u001b[39m=\u001b[39m Gemini(model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/gemini-pro\u001b[39m\u001b[39m'\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#X66sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     embed_model \u001b[39m=\u001b[39m GeminiEmbedding(api_key\u001b[39m=\u001b[39mGOOGLE_API_KEY)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\llama_index\\vector_stores\\google\\generativeai\\base.py:213\u001b[0m, in \u001b[0;36mGoogleVectorStore.create_corpus\u001b[1;34m(cls, corpus_id, display_name)\u001b[0m\n\u001b[0;32m    211\u001b[0m client \u001b[39m=\u001b[39m genaix\u001b[39m.\u001b[39mbuild_semantic_retriever()\n\u001b[0;32m    212\u001b[0m new_corpus_id \u001b[39m=\u001b[39m corpus_id \u001b[39mor\u001b[39;00m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4())\n\u001b[1;32m--> 213\u001b[0m new_corpus \u001b[39m=\u001b[39m genaix\u001b[39m.\u001b[39;49mcreate_corpus(\n\u001b[0;32m    214\u001b[0m     corpus_id\u001b[39m=\u001b[39;49mnew_corpus_id, display_name\u001b[39m=\u001b[39;49mdisplay_name, client\u001b[39m=\u001b[39;49mclient\n\u001b[0;32m    215\u001b[0m )\n\u001b[0;32m    216\u001b[0m name \u001b[39m=\u001b[39m genaix\u001b[39m.\u001b[39mEntityName\u001b[39m.\u001b[39mfrom_str(new_corpus\u001b[39m.\u001b[39mname)\n\u001b[0;32m    217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(corpus_id\u001b[39m=\u001b[39mname\u001b[39m.\u001b[39mcorpus_id, client\u001b[39m=\u001b[39mclient)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\llama_index\\vector_stores\\google\\generativeai\\genai_extension.py:281\u001b[0m, in \u001b[0;36mcreate_corpus\u001b[1;34m(corpus_id, display_name, client)\u001b[0m\n\u001b[0;32m    277\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    279\u001b[0m new_display_name \u001b[39m=\u001b[39m display_name \u001b[39mor\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUntitled \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 281\u001b[0m new_corpus \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate_corpus(\n\u001b[0;32m    282\u001b[0m     genai\u001b[39m.\u001b[39;49mCreateCorpusRequest(\n\u001b[0;32m    283\u001b[0m         corpus\u001b[39m=\u001b[39;49mgenai\u001b[39m.\u001b[39;49mCorpus(name\u001b[39m=\u001b[39;49mname, display_name\u001b[39m=\u001b[39;49mnew_display_name)\n\u001b[0;32m    284\u001b[0m     )\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m Corpus\u001b[39m.\u001b[39mfrom_corpus(new_corpus)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\retriever_service\\client.py:566\u001b[0m, in \u001b[0;36mRetrieverServiceClient.create_corpus\u001b[1;34m(self, request, corpus, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    563\u001b[0m rpc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39m_wrapped_methods[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport\u001b[39m.\u001b[39mcreate_corpus]\n\u001b[0;32m    565\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 566\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[0;32m    567\u001b[0m     request,\n\u001b[0;32m    568\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[0;32m    569\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    570\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    573\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    369\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    371\u001b[0m )\n\u001b[1;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    373\u001b[0m     target,\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    375\u001b[0m     sleep_generator,\n\u001b[0;32m    376\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    377\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    378\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[0;32m    206\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m         result \u001b[39m=\u001b[39m target()\n\u001b[0;32m    208\u001b[0m         \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39misawaitable(result):\n\u001b[0;32m    209\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout \u001b[39m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:81\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Project has the maximum number of Corpora (5)."
     ]
    }
   ],
   "source": [
    "generate_vector_embedding_google(index_name, './semantics/models/semantic_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric layer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"metric_layer\"\n",
    "index_name = project_name.lower() + '_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  metric_layer_embeddings\n",
      "semantics\\models\\metrics\\discount_amount_average.yml\n",
      "semantics\\models\\metrics\\discount_amount_total.yml\n",
      "semantics\\models\\metrics\\price_average.yml\n",
      "semantics\\models\\metrics\\price_total.yml\n",
      "semantics\\models\\metrics\\product_cost_average.yml\n",
      "semantics\\models\\metrics\\product_cost_total.yml\n",
      "semantics\\models\\metrics\\quantity_average.yml\n",
      "semantics\\models\\metrics\\quantity_total.yml\n",
      "semantics\\models\\metrics\\revenue_average.yml\n",
      "semantics\\models\\metrics\\revenue_total.yml\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\discount_amount_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"11388b69-5857-4ae0-af93-ab89040171d3\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"a8c7524e-73bf-4d58-9e6a-454cbe48e49e\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"8e955d62a41cdd5f702b03c81959c90ba3677a8ddf29c92e7cda5e4d805b4aac\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"6ffd5d4a-51d4-4bb3-b35e-a3690382ae6f\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"c08f3f818402c1a16e8a6de33d082f7d84b192daa6aebf041fea42f5e61d03e0\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 207, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'a8c7524e-73bf-4d58-9e6a-454cbe48e49e')\n",
      "('doc_id', 'a8c7524e-73bf-4d58-9e6a-454cbe48e49e')\n",
      "('ref_doc_id', 'a8c7524e-73bf-4d58-9e6a-454cbe48e49e')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\discount_amount_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"6ffd5d4a-51d4-4bb3-b35e-a3690382ae6f\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"d5f5de20-69b1-42a6-8b2f-198e81693682\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"0554151107ba98231ebdc01d8e9c64327aaa8d3661b0810b6cea8d542927cf3f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"11388b69-5857-4ae0-af93-ab89040171d3\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"c08f3f818402c1a16e8a6de33d082f7d84b192daa6aebf041fea42f5e61d03e0\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"ec20a7a3-623a-4c35-920f-1b913f35f0f4\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 201, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'd5f5de20-69b1-42a6-8b2f-198e81693682')\n",
      "('doc_id', 'd5f5de20-69b1-42a6-8b2f-198e81693682')\n",
      "('ref_doc_id', 'd5f5de20-69b1-42a6-8b2f-198e81693682')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\price_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"ec20a7a3-623a-4c35-920f-1b913f35f0f4\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"12547796-97d3-437c-ba5c-5543f912f129\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"8529d9ff8e8ea7ef9911a902de9ddcf7e747db7605aa971e25cf9916a639e278\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"6ffd5d4a-51d4-4bb3-b35e-a3690382ae6f\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"41137b66-98d4-43d2-a6cd-2d75bc5f58a1\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 174, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '12547796-97d3-437c-ba5c-5543f912f129')\n",
      "('doc_id', '12547796-97d3-437c-ba5c-5543f912f129')\n",
      "('ref_doc_id', '12547796-97d3-437c-ba5c-5543f912f129')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\price_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"41137b66-98d4-43d2-a6cd-2d75bc5f58a1\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"5186df1e-6e35-46e2-bbb1-c4574d629e68\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"1fcc5bbd05eae68adfb1f71364b345021578bf646af1143db6370d12f670f000\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"ec20a7a3-623a-4c35-920f-1b913f35f0f4\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"510c5064-6cd9-4a65-b0b9-e6e915e5abaf\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 164, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '5186df1e-6e35-46e2-bbb1-c4574d629e68')\n",
      "('doc_id', '5186df1e-6e35-46e2-bbb1-c4574d629e68')\n",
      "('ref_doc_id', '5186df1e-6e35-46e2-bbb1-c4574d629e68')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\product_cost_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"510c5064-6cd9-4a65-b0b9-e6e915e5abaf\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"3e1cb247-d0de-4ae6-a762-26e541ed1303\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"58f7914cce1d638d356a7abe8a2c2aea277a7cb5433c8d0a39346977afe4a86d\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"41137b66-98d4-43d2-a6cd-2d75bc5f58a1\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"ce4266be-707d-4cce-aeb5-47fd899fc732\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 195, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '3e1cb247-d0de-4ae6-a762-26e541ed1303')\n",
      "('doc_id', '3e1cb247-d0de-4ae6-a762-26e541ed1303')\n",
      "('ref_doc_id', '3e1cb247-d0de-4ae6-a762-26e541ed1303')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\product_cost_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"ce4266be-707d-4cce-aeb5-47fd899fc732\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"d7aedd3a-5126-480d-ad17-b73e96a989c4\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"0e809e2b0bed64acc560539f074dc65f906571e9308d2cd777a58831e77684a1\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"510c5064-6cd9-4a65-b0b9-e6e915e5abaf\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"83afbad1-3eb7-4409-84f8-21fd64ae695d\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 185, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'd7aedd3a-5126-480d-ad17-b73e96a989c4')\n",
      "('doc_id', 'd7aedd3a-5126-480d-ad17-b73e96a989c4')\n",
      "('ref_doc_id', 'd7aedd3a-5126-480d-ad17-b73e96a989c4')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\quantity_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"83afbad1-3eb7-4409-84f8-21fd64ae695d\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"6b5f2fbb-328f-4103-b4f8-beb4a8dc2835\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"afb40b9bb796543ec4cd65865c30ada094f780f8326451ae6f785e1708866271\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"ce4266be-707d-4cce-aeb5-47fd899fc732\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"442bde63-a287-4c96-a53f-b1953bf99775\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 186, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '6b5f2fbb-328f-4103-b4f8-beb4a8dc2835')\n",
      "('doc_id', '6b5f2fbb-328f-4103-b4f8-beb4a8dc2835')\n",
      "('ref_doc_id', '6b5f2fbb-328f-4103-b4f8-beb4a8dc2835')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\quantity_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"442bde63-a287-4c96-a53f-b1953bf99775\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"36f2e472-4a75-4ee3-993b-718773fae7ad\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"006a9924ec0b2803849ddfc03a17ef3b7dbb59b7e48b44c13b78040dcd07e2cd\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"83afbad1-3eb7-4409-84f8-21fd64ae695d\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"7da05e8d-3262-4d40-af4b-570d56755f06\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 182, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '36f2e472-4a75-4ee3-993b-718773fae7ad')\n",
      "('doc_id', '36f2e472-4a75-4ee3-993b-718773fae7ad')\n",
      "('ref_doc_id', '36f2e472-4a75-4ee3-993b-718773fae7ad')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\revenue_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"7da05e8d-3262-4d40-af4b-570d56755f06\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"163fc716-0637-4ddc-9139-afc13705800e\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"547fd07529d40a6c9ed21d2838133f6ef0d7e74fff03a5e0d5848ca2527e37b1\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"442bde63-a287-4c96-a53f-b1953bf99775\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"c99f2a19-6270-4ef3-a849-fe7b84a23fbb\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"dfcd54929816ce3c9e8a524d7c5fd9d9d02ab53392d6b7fb13573c93f110af38\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 202, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '163fc716-0637-4ddc-9139-afc13705800e')\n",
      "('doc_id', '163fc716-0637-4ddc-9139-afc13705800e')\n",
      "('ref_doc_id', '163fc716-0637-4ddc-9139-afc13705800e')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\revenue_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"c99f2a19-6270-4ef3-a849-fe7b84a23fbb\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"d730b15c-fce5-4087-ba7d-039d14125194\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"fb7094fa25461ac86aeb2ca26384aa107438a83d173bfd8a38abb18175bdbeec\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"7da05e8d-3262-4d40-af4b-570d56755f06\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"dfcd54929816ce3c9e8a524d7c5fd9d9d02ab53392d6b7fb13573c93f110af38\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 194, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'd730b15c-fce5-4087-ba7d-039d14125194')\n",
      "('doc_id', 'd730b15c-fce5-4087-ba7d-039d14125194')\n",
      "('ref_doc_id', 'd730b15c-fce5-4087-ba7d-039d14125194')\n",
      "Vector embeddings created in 14.432640790939331 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'status': 'Chroma embedding complete'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vector_embedding(index_name, './semantics/models/metrics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  metric_layer_embeddings\n",
      "semantics\\models\\marts\\customers.sql\n",
      "semantics\\models\\marts\\orders.sql\n",
      "semantics\\models\\marts\\products.sql\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\customers.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"5cbc9e54-2cb2-4e3b-9b89-5481fa69e991\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"1864f422-3536-4520-a0ee-b3502ade74a6\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"hash\": \"7f9e93ad9eaa4628059a72fd5a30c3bd3c92af45eb810c41c7fa5ae29280598d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"d8fddc05-5af9-4a02-bae9-01cbe670904e\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"aa65dde83abdac2f51ea942211a236a55a9f4e81015516dd0ae4352a27d34124\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 746, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '1864f422-3536-4520-a0ee-b3502ade74a6')\n",
      "('doc_id', '1864f422-3536-4520-a0ee-b3502ade74a6')\n",
      "('ref_doc_id', '1864f422-3536-4520-a0ee-b3502ade74a6')\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\orders.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"d8fddc05-5af9-4a02-bae9-01cbe670904e\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"c9e1f17c-3edd-44bd-b08c-6c54bc4e180a\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"hash\": \"58a9260395b0fc7ddd411e23e844e16309eb0dae14d3dd3b392e2c61e9e37f3f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"5cbc9e54-2cb2-4e3b-9b89-5481fa69e991\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"hash\": \"aa65dde83abdac2f51ea942211a236a55a9f4e81015516dd0ae4352a27d34124\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"b52a7121-341d-4c1d-b868-78f526095554\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"061d7edb5507d47c686da40cba1a281d08991343b87192a13c777a147f8cc25b\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 930, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'c9e1f17c-3edd-44bd-b08c-6c54bc4e180a')\n",
      "('doc_id', 'c9e1f17c-3edd-44bd-b08c-6c54bc4e180a')\n",
      "('ref_doc_id', 'c9e1f17c-3edd-44bd-b08c-6c54bc4e180a')\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\products.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"b52a7121-341d-4c1d-b868-78f526095554\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\products.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"960ed2aa-13b8-48f2-800d-e15571b76abc\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\products.sql\", \"file_type\": \".sql\"}, \"hash\": \"94f19120b1faf533d8c98651c49229bfe5ec38fd6d1a540599d201d76dc1b89f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"d8fddc05-5af9-4a02-bae9-01cbe670904e\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"061d7edb5507d47c686da40cba1a281d08991343b87192a13c777a147f8cc25b\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 900, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '960ed2aa-13b8-48f2-800d-e15571b76abc')\n",
      "('doc_id', '960ed2aa-13b8-48f2-800d-e15571b76abc')\n",
      "('ref_doc_id', '960ed2aa-13b8-48f2-800d-e15571b76abc')\n",
      "Vector embeddings created in 6.426392555236816 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'status': 'Chroma embedding complete'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vector_embedding(index_name, './semantics/models/marts/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic and metric layers embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"semantic_metric_layer\"\n",
    "index_name = project_name.lower() + '_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  semantic_metric_layer_embeddings\n",
      "semantics\\models\\marts\\customers.sql\n",
      "semantics\\models\\marts\\orders.sql\n",
      "semantics\\models\\marts\\products.sql\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\customers.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"126c0ec9-1157-448e-b50d-bc1e1ab036ae\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"c4ee99b2-11e6-449c-975d-1195d11bc436\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"hash\": \"7f9e93ad9eaa4628059a72fd5a30c3bd3c92af45eb810c41c7fa5ae29280598d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"e729e098-3e45-462f-bb97-c638c83445a8\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"aa65dde83abdac2f51ea942211a236a55a9f4e81015516dd0ae4352a27d34124\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 746, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'c4ee99b2-11e6-449c-975d-1195d11bc436')\n",
      "('doc_id', 'c4ee99b2-11e6-449c-975d-1195d11bc436')\n",
      "('ref_doc_id', 'c4ee99b2-11e6-449c-975d-1195d11bc436')\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\orders.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"e729e098-3e45-462f-bb97-c638c83445a8\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"50d22f40-a247-4e14-9a6e-3277aff25cee\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"hash\": \"58a9260395b0fc7ddd411e23e844e16309eb0dae14d3dd3b392e2c61e9e37f3f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"126c0ec9-1157-448e-b50d-bc1e1ab036ae\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\customers.sql\", \"file_type\": \".sql\"}, \"hash\": \"aa65dde83abdac2f51ea942211a236a55a9f4e81015516dd0ae4352a27d34124\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"632061ae-d9e8-4a24-821e-2241d2f801ab\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"061d7edb5507d47c686da40cba1a281d08991343b87192a13c777a147f8cc25b\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 930, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '50d22f40-a247-4e14-9a6e-3277aff25cee')\n",
      "('doc_id', '50d22f40-a247-4e14-9a6e-3277aff25cee')\n",
      "('ref_doc_id', '50d22f40-a247-4e14-9a6e-3277aff25cee')\n",
      "('filename', 'semantics\\\\models\\\\marts\\\\products.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"632061ae-d9e8-4a24-821e-2241d2f801ab\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\products.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"5b556a57-9999-44b0-ba9d-8f15443bc49e\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\products.sql\", \"file_type\": \".sql\"}, \"hash\": \"94f19120b1faf533d8c98651c49229bfe5ec38fd6d1a540599d201d76dc1b89f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"e729e098-3e45-462f-bb97-c638c83445a8\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\marts\\\\\\\\orders.sql\", \"file_type\": \".sql\"}, \"hash\": \"5b54e04c765eca699b4fd62cf56ecb41cd72c66e3f843988eb57980f6dca0399\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"061d7edb5507d47c686da40cba1a281d08991343b87192a13c777a147f8cc25b\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 900, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '5b556a57-9999-44b0-ba9d-8f15443bc49e')\n",
      "('doc_id', '5b556a57-9999-44b0-ba9d-8f15443bc49e')\n",
      "('ref_doc_id', '5b556a57-9999-44b0-ba9d-8f15443bc49e')\n",
      "Vector embeddings created in 6.608978033065796 seconds.\n",
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  semantic_metric_layer_embeddings\n",
      "semantics\\models\\metrics\\discount_amount_average.yml\n",
      "semantics\\models\\metrics\\discount_amount_total.yml\n",
      "semantics\\models\\metrics\\price_average.yml\n",
      "semantics\\models\\metrics\\price_total.yml\n",
      "semantics\\models\\metrics\\product_cost_average.yml\n",
      "semantics\\models\\metrics\\product_cost_total.yml\n",
      "semantics\\models\\metrics\\quantity_average.yml\n",
      "semantics\\models\\metrics\\quantity_total.yml\n",
      "semantics\\models\\metrics\\revenue_average.yml\n",
      "semantics\\models\\metrics\\revenue_total.yml\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\discount_amount_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"04787172-963b-4666-b8ef-861524ed8d19\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"2775c499-f117-4814-8cd2-0b32da41d532\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"8e955d62a41cdd5f702b03c81959c90ba3677a8ddf29c92e7cda5e4d805b4aac\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"0ead22f7-deac-4ca7-b0fe-d3d9eb3aaed9\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"c08f3f818402c1a16e8a6de33d082f7d84b192daa6aebf041fea42f5e61d03e0\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 207, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '2775c499-f117-4814-8cd2-0b32da41d532')\n",
      "('doc_id', '2775c499-f117-4814-8cd2-0b32da41d532')\n",
      "('ref_doc_id', '2775c499-f117-4814-8cd2-0b32da41d532')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\discount_amount_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"0ead22f7-deac-4ca7-b0fe-d3d9eb3aaed9\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"84e982b2-f094-4214-9907-ee0a69317a26\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"0554151107ba98231ebdc01d8e9c64327aaa8d3661b0810b6cea8d542927cf3f\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"04787172-963b-4666-b8ef-861524ed8d19\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"c08f3f818402c1a16e8a6de33d082f7d84b192daa6aebf041fea42f5e61d03e0\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"463d9e1e-04f0-47c8-826b-7d700f5ce8dd\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 201, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '84e982b2-f094-4214-9907-ee0a69317a26')\n",
      "('doc_id', '84e982b2-f094-4214-9907-ee0a69317a26')\n",
      "('ref_doc_id', '84e982b2-f094-4214-9907-ee0a69317a26')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\price_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"463d9e1e-04f0-47c8-826b-7d700f5ce8dd\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"2a249afc-28a9-4afe-b61f-c463f780112a\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"8529d9ff8e8ea7ef9911a902de9ddcf7e747db7605aa971e25cf9916a639e278\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"0ead22f7-deac-4ca7-b0fe-d3d9eb3aaed9\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\discount_amount_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"916b8d79d5c858879f2744f459ecdcef75b873a795ae594023166014e337a3df\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"cf5550ab-a11d-42e3-bc96-3312b682d57f\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 174, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '2a249afc-28a9-4afe-b61f-c463f780112a')\n",
      "('doc_id', '2a249afc-28a9-4afe-b61f-c463f780112a')\n",
      "('ref_doc_id', '2a249afc-28a9-4afe-b61f-c463f780112a')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\price_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"cf5550ab-a11d-42e3-bc96-3312b682d57f\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"36e816d3-92bb-4df2-a4bc-f417af9b9d03\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"1fcc5bbd05eae68adfb1f71364b345021578bf646af1143db6370d12f670f000\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"463d9e1e-04f0-47c8-826b-7d700f5ce8dd\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"60b3150761568282ffb2da2497d39051220d6e2fd32b95948922ab2d1113eafb\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"25d9d58a-4997-4fe3-bd50-86e2c0851279\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 164, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '36e816d3-92bb-4df2-a4bc-f417af9b9d03')\n",
      "('doc_id', '36e816d3-92bb-4df2-a4bc-f417af9b9d03')\n",
      "('ref_doc_id', '36e816d3-92bb-4df2-a4bc-f417af9b9d03')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\product_cost_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"25d9d58a-4997-4fe3-bd50-86e2c0851279\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"484e15b4-8d8c-4623-9dff-2ae040b29281\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"58f7914cce1d638d356a7abe8a2c2aea277a7cb5433c8d0a39346977afe4a86d\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"cf5550ab-a11d-42e3-bc96-3312b682d57f\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\price_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"2103acd6b8bbfd97eb3e0cb429fe1c49ab6aee42e5a279b43559ba06f95dde87\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"cf01079b-3696-48f1-8b53-70e913d4f400\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 195, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '484e15b4-8d8c-4623-9dff-2ae040b29281')\n",
      "('doc_id', '484e15b4-8d8c-4623-9dff-2ae040b29281')\n",
      "('ref_doc_id', '484e15b4-8d8c-4623-9dff-2ae040b29281')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\product_cost_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"cf01079b-3696-48f1-8b53-70e913d4f400\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"c57451d9-4c7d-4ff4-bec8-846f5c60a6cc\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"0e809e2b0bed64acc560539f074dc65f906571e9308d2cd777a58831e77684a1\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"25d9d58a-4997-4fe3-bd50-86e2c0851279\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"79c6c6b2def13730facb7a4cf04600082ddd711a61b5c7290853bca2b69831b4\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"dbd58776-7854-4071-9e9d-7931a0ae86cf\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 185, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'c57451d9-4c7d-4ff4-bec8-846f5c60a6cc')\n",
      "('doc_id', 'c57451d9-4c7d-4ff4-bec8-846f5c60a6cc')\n",
      "('ref_doc_id', 'c57451d9-4c7d-4ff4-bec8-846f5c60a6cc')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\quantity_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"dbd58776-7854-4071-9e9d-7931a0ae86cf\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"4df4a04c-9362-4bd6-a2e3-988c3f70f108\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"afb40b9bb796543ec4cd65865c30ada094f780f8326451ae6f785e1708866271\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"cf01079b-3696-48f1-8b53-70e913d4f400\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\product_cost_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"a132c073d20abac02212ec6b5eaca7b477db50190ec21347cb9600d87abec294\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"59595d63-3980-4435-bd84-b499e390721d\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 186, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '4df4a04c-9362-4bd6-a2e3-988c3f70f108')\n",
      "('doc_id', '4df4a04c-9362-4bd6-a2e3-988c3f70f108')\n",
      "('ref_doc_id', '4df4a04c-9362-4bd6-a2e3-988c3f70f108')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\quantity_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"59595d63-3980-4435-bd84-b499e390721d\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"f24bfd5d-7dca-4453-89b4-bb2258c28eaa\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"006a9924ec0b2803849ddfc03a17ef3b7dbb59b7e48b44c13b78040dcd07e2cd\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"dbd58776-7854-4071-9e9d-7931a0ae86cf\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"e6dcf0af086de9dc72f4cd82d5b9c3983e943b043120b7a32c8321535e08631d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"70548c84-60cc-4ea1-b88c-30778a76c15c\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 182, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'f24bfd5d-7dca-4453-89b4-bb2258c28eaa')\n",
      "('doc_id', 'f24bfd5d-7dca-4453-89b4-bb2258c28eaa')\n",
      "('ref_doc_id', 'f24bfd5d-7dca-4453-89b4-bb2258c28eaa')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\revenue_average.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"70548c84-60cc-4ea1-b88c-30778a76c15c\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"f32aeaa3-5a31-4b4f-96c6-171a2793ae80\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"547fd07529d40a6c9ed21d2838133f6ef0d7e74fff03a5e0d5848ca2527e37b1\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"59595d63-3980-4435-bd84-b499e390721d\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\quantity_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"49950b999dd1c0694083813525439c1245d337e754a5ee0944aab9a33058104b\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"40f6af33-5a7a-40ef-9ab8-ffa0dfeb1be0\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"dfcd54929816ce3c9e8a524d7c5fd9d9d02ab53392d6b7fb13573c93f110af38\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 202, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'f32aeaa3-5a31-4b4f-96c6-171a2793ae80')\n",
      "('doc_id', 'f32aeaa3-5a31-4b4f-96c6-171a2793ae80')\n",
      "('ref_doc_id', 'f32aeaa3-5a31-4b4f-96c6-171a2793ae80')\n",
      "('filename', 'semantics\\\\models\\\\metrics\\\\revenue_total.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"40f6af33-5a7a-40ef-9ab8-ffa0dfeb1be0\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_total.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"485f58c4-e21d-4e76-b81f-3f9d78a0028d\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_total.yml\", \"file_type\": \".yml\"}, \"hash\": \"fb7094fa25461ac86aeb2ca26384aa107438a83d173bfd8a38abb18175bdbeec\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"70548c84-60cc-4ea1-b88c-30778a76c15c\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\metrics\\\\\\\\revenue_average.yml\", \"file_type\": \".yml\"}, \"hash\": \"91619805a5d4ba0306658c45560fb3e99aa891eb3185dae386a8ee22df24a3c7\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"dfcd54929816ce3c9e8a524d7c5fd9d9d02ab53392d6b7fb13573c93f110af38\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 194, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '485f58c4-e21d-4e76-b81f-3f9d78a0028d')\n",
      "('doc_id', '485f58c4-e21d-4e76-b81f-3f9d78a0028d')\n",
      "('ref_doc_id', '485f58c4-e21d-4e76-b81f-3f9d78a0028d')\n",
      "Vector embeddings created in 11.866422176361084 seconds.\n",
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  semantic_metric_layer_embeddings\n",
      "semantics\\models\\semantic_models\\customers.yml\n",
      "semantics\\models\\semantic_models\\metricflow_time_spine.sql\n",
      "semantics\\models\\semantic_models\\orders.yml\n",
      "semantics\\models\\semantic_models\\products.yml\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\customers.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"8e6392fe-e2e1-4217-aac0-232129f34a83\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"42a299b7-bfa7-486d-8a98-cc5678e11249\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"hash\": \"c277a6f8f700e10e4ddaa1e1b8d1006815a3e7cab07f95921bcc785e9a505384\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"566d7d96-14fe-4e9f-8a66-47839d197023\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"6e7553c9d005d1e21c4acaf6158bc0cdef0bec395d3db52085b97bc68c0e62ae\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 654, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', '42a299b7-bfa7-486d-8a98-cc5678e11249')\n",
      "('doc_id', '42a299b7-bfa7-486d-8a98-cc5678e11249')\n",
      "('ref_doc_id', '42a299b7-bfa7-486d-8a98-cc5678e11249')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\metricflow_time_spine.sql')\n",
      "('file_type', '.sql')\n",
      "('_node_content', '{\"id_\": \"566d7d96-14fe-4e9f-8a66-47839d197023\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"f9a461fe-5a94-4656-99c5-12d040e3d56c\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"hash\": \"5ab69159a18e468f0159c267f179366bdf5d1cf5957532107a0e93e9b1a36e0c\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"8e6392fe-e2e1-4217-aac0-232129f34a83\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\customers.yml\", \"file_type\": \".yml\"}, \"hash\": \"6e7553c9d005d1e21c4acaf6158bc0cdef0bec395d3db52085b97bc68c0e62ae\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"d4697afa-b74d-41d8-99c7-3809526e291e\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 262, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'f9a461fe-5a94-4656-99c5-12d040e3d56c')\n",
      "('doc_id', 'f9a461fe-5a94-4656-99c5-12d040e3d56c')\n",
      "('ref_doc_id', 'f9a461fe-5a94-4656-99c5-12d040e3d56c')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\orders.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"d4697afa-b74d-41d8-99c7-3809526e291e\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"d662da66-5bfd-44c0-8736-7e674c3130d0\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"hash\": \"c267a8292f6eedc940c949df963bd1045086dd47275392af78aa08adb86e3748\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"566d7d96-14fe-4e9f-8a66-47839d197023\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\metricflow_time_spine.sql\", \"file_type\": \".sql\"}, \"hash\": \"8148c6328936adf42707083016cedf8e2e81e41a9e9d9fbcce073d0aef982d1d\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"41c35748-5b80-4204-b9f7-32f0f35d296b\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"c8cecc2bf7d8d1568c416b437fd4b2f2c59d1e4f39bb2209c0a746f5efc9f1d8\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 1778, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'd662da66-5bfd-44c0-8736-7e674c3130d0')\n",
      "('doc_id', 'd662da66-5bfd-44c0-8736-7e674c3130d0')\n",
      "('ref_doc_id', 'd662da66-5bfd-44c0-8736-7e674c3130d0')\n",
      "('filename', 'semantics\\\\models\\\\semantic_models\\\\products.yml')\n",
      "('file_type', '.yml')\n",
      "('_node_content', '{\"id_\": \"41c35748-5b80-4204-b9f7-32f0f35d296b\", \"embedding\": null, \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\products.yml\", \"file_type\": \".yml\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"a83fecf7-efc6-4859-9207-f4cf69b3a118\", \"node_type\": \"4\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\products.yml\", \"file_type\": \".yml\"}, \"hash\": \"8ea46e1942baf1a3a56cc434194b57174ff402caeca30125960dfa203306df61\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"d4697afa-b74d-41d8-99c7-3809526e291e\", \"node_type\": \"1\", \"metadata\": {\"filename\": \"semantics\\\\\\\\models\\\\\\\\semantic_models\\\\\\\\orders.yml\", \"file_type\": \".yml\"}, \"hash\": \"6ae2d741d27353ae3e41e2db50fa26c4f37a11cdb97385aa6ad936b5a4360a40\", \"class_name\": \"RelatedNodeInfo\"}}, \"hash\": \"c8cecc2bf7d8d1568c416b437fd4b2f2c59d1e4f39bb2209c0a746f5efc9f1d8\", \"text\": \"\", \"start_char_idx\": 0, \"end_char_idx\": 675, \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"metadata_template\": \"{key}: {value}\", \"metadata_seperator\": \"\\\\n\", \"class_name\": \"TextNode\"}')\n",
      "('_node_type', 'TextNode')\n",
      "('document_id', 'a83fecf7-efc6-4859-9207-f4cf69b3a118')\n",
      "('doc_id', 'a83fecf7-efc6-4859-9207-f4cf69b3a118')\n",
      "('ref_doc_id', 'a83fecf7-efc6-4859-9207-f4cf69b3a118')\n",
      "Vector embeddings created in 6.799181699752808 seconds.\n"
     ]
    }
   ],
   "source": [
    "for i in [\"marts\",\"metrics\",\"semantic_models\"]:\n",
    "    generate_vector_embedding(index_name, f'./semantics/models/{i}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_thread(memory, reset=None):\n",
    "    if reset:\n",
    "        new_conversation_state = init_chat_history()\n",
    "        return new_conversation_state\n",
    "    return memory\n",
    "\n",
    "def init_chat_history():\n",
    "    new_conversation_state = ChatMemoryBuffer.from_defaults(token_limit=50000)\n",
    "    return new_conversation_state\n",
    "\n",
    "\n",
    "chat_history = init_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing knowledge base size... 17\n",
      "Using Gemini Pro...\n",
      "Retrieving knowledge base index from ChromaDB...\n",
      "Index retrieved from ChromaDB in 1.403778076171875 seconds.\n",
      "Starting response stream...\n",
      "...........................\n",
      "...........................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"total revenue?\"\n",
    "answer_query_stream(query, index_name, chat_history, semantic_prompt_style())\n",
    "    # for token in response.response_gen:\n",
    "    # print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = init_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing knowledge base size... 17\n",
      "Using Gemini Pro...\n",
      "Retrieving knowledge base index from ChromaDB...\n",
      "Index retrieved from ChromaDB in 2.2684946060180664 seconds.\n",
      "Starting response stream...\n",
      "...........................\n",
      "...........................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'query --metrics revenue_total --group-by sales_key__order_date --where \"strftime(\\'%m\\', sales_key__order_date) = \\'01\\'\"'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"semantic_metric_layer\"\n",
    "index_name = project_name.lower() + '_embeddings'\n",
    "\n",
    "query = 'how much did we make in January?'\n",
    "mf_command = answer_query_stream(query, index_name, chat_history, query_gen_prompt_style())\n",
    "mf_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa6\\r\\r/ Initiating query\\xe2\\x80\\xa6\\r\\r- Initiating query\\xe2\\x80\\xa6\\r\\r\\\\ Initiating query\\xe2\\x80\\xa6\\r\\r| Initiating query\\xe2\\x80\\xa616:48:46  BigQuery adapter: https://console.cloud.google.com/bigquery?project=alphabeam&j=bq:US:1753656a-f743-480f-819b-94b2c1932dfd&page=queryresults\\r\\n\\r\\nERROR: Database Error\\r\\n  Function not found: strftime at [17:7]\\r\\nLog file: c:\\\\Users\\\\HP\\\\Zion\\\\alphabeam\\\\src\\\\semantics\\\\logs\\\\metricflow.log\\r\\n\\r'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Zion\\alphabeam\\src\\semantic_retrieval.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fetch_data(query, mf_command)\n",
      "\u001b[1;32mc:\\Users\\HP\\Zion\\alphabeam\\src\\semantic_retrieval.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# doc.doc_id = \"llm-output\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# node_parser = SimpleNodeParser.from_defaults(\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     chunk_size=1024,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#     chunk_overlap=20\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#     )\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m service_context \u001b[39m=\u001b[39m ServiceContext\u001b[39m.\u001b[39mfrom_defaults(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     llm\u001b[39m=\u001b[39mGemini(model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/gemini-pro\u001b[39m\u001b[39m'\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     embed_model\u001b[39m=\u001b[39mGeminiEmbedding(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     chunk_overlap\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m index \u001b[39m=\u001b[39m VectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(doc, service_context\u001b[39m=\u001b[39;49mservice_context)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m chat_engine \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mas_chat_engine(chat_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                                     memory\u001b[39m=\u001b[39mchat_history,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                     system_prompt\u001b[39m=\u001b[39mreturned_data_prompt_style(), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                                     function_call\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mquery_engine_tool\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/HP/Zion/alphabeam/src/semantic_retrieval.ipynb#Y110sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m message_body \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mUse the tool to answer:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00muser_query\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\alphabeam\\lib\\site-packages\\llama_index\\indices\\base.py:97\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m service_context\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mas_trace(\u001b[39m\"\u001b[39m\u001b[39mindex_construction\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     96\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents:\n\u001b[1;32m---> 97\u001b[0m         docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39;49mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[0;32m     99\u001b[0m     nodes \u001b[39m=\u001b[39m run_transformations(\n\u001b[0;32m    100\u001b[0m         documents,  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         service_context\u001b[39m.\u001b[39mtransformations,\n\u001b[0;32m    102\u001b[0m         show_progress\u001b[39m=\u001b[39mshow_progress,\n\u001b[0;32m    103\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    107\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m    108\u001b[0m         storage_context\u001b[39m=\u001b[39mstorage_context,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    112\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    "fetch_data(query, mf_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Tru, TruLlama\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI\n",
    "\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_stream(query, index_name):\n",
    "\n",
    "    index, doc_size = get_index_from_vector_db(index_name)\n",
    "\n",
    "    # node_postprocessors = postprocessor_args(doc_size)\n",
    "    similarity_top_k = 200 if doc_size>200 else doc_size\n",
    "    chat_engine = index.as_chat_engine(chat_mode=\"context\", \n",
    "                                        system_prompt=query_gen_prompt_style(), \n",
    "                                        similarity_top_k=similarity_top_k,\n",
    "                                        verbose=True, \n",
    "                                        # streaming=True,\n",
    "                                        function_call=\"query_engine_tool\",\n",
    "                                        )\n",
    "\n",
    "    message_body = f\"\"\"\\nUse the tool to answer:\\n{query}\\n\"\"\"\n",
    "    response = chat_engine.chat(message_body)\n",
    "    return index, response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing knowledge base size... 17\n",
      "Using Gemini Pro...\n",
      "Retrieving knowledge base index from ChromaDB...\n",
      "Index retrieved from ChromaDB in 1.0494778156280518 seconds.\n",
      "query --metrics quantity_total --group-by order_date --where \"ship_date IS NOT NULL\"\n"
     ]
    }
   ],
   "source": [
    "query = 'how any orders have been shipped'\n",
    "# mf_command = answer_query_stream(query, \"semantic_metric_layer_embeddings\", chat_history, query_gen_prompt_style())\n",
    "\n",
    "index, response = answer_query_stream(query, 'semantic_metric_layer_embeddings')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
      "✅ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=OpenAI())\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = Feedback(grounded.groundedness_measure_with_cot_reasons).on(\n",
    "    TruLlama.select_source_nodes().node.text.collect()\n",
    "    ).on_output(\n",
    "    ).aggregate(grounded.grounded_statements_aggregator)\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = Feedback(openai.relevance).on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_qs_relevance = Feedback(openai.qs_relevance).on_input().on(\n",
    "    TruLlama.select_source_nodes().node.text\n",
    "    ).aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_query_engine_recorder = TruLlama(query_engine,\n",
    "    app_id='LlamaIndex_App1',\n",
    "    feedbacks=[f_groundedness, f_qa_relevance, f_qs_relevance])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x2c177c93370 is calling an instrumented method <function BaseQueryEngine.query at 0x000002C168B75BD0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x2c174543d30) using this function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new object of type <class 'llama_index.query_engine.retriever_query_engine.RetrieverQueryEngine'> at 0x2c177c93370 is calling an instrumented method <function RetrieverQueryEngine.retrieve at 0x000002C16D620DC0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app based on other object (0x2c174543d30) using this function.\n",
      "A new object of type <class 'llama_index.indices.vector_store.retrievers.retriever.VectorIndexRetriever'> at 0x2c175a84340 is calling an instrumented method <function BaseRetriever.retrieve at 0x000002C168B74E50>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app.retriever based on other object (0x2c174543340) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x2c175b5f0d0 is calling an instrumented method <function BaseSynthesizer.synthesize at 0x000002C1698593F0>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x2c1745435e0) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x2c175b5f0d0 is calling an instrumented method <function CompactAndRefine.get_response at 0x000002C169858940>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x2c1745435e0) using this function.\n",
      "A new object of type <class 'llama_index.response_synthesizers.compact_and_refine.CompactAndRefine'> at 0x2c175b5f0d0 is calling an instrumented method <function Refine.get_response at 0x000002C16A1A3250>. The path of this call may be incorrect.\n",
      "Guessing path of new object is app._response_synthesizer based on other object (0x2c1745435e0) using this function.\n"
     ]
    }
   ],
   "source": [
    "# or as context manager\n",
    "query_engine = index.as_query_engine(system_prompt=query_gen_prompt_style(),\n",
    "                                        similarity_top_k=10,\n",
    "                                        function_call=\"query_engine_tool\",)\n",
    "with tru_query_engine_recorder as recording:\n",
    "    query_engine.query(\"how any orders have been shipped?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>relevance</th>\n",
       "      <th>groundedness_measure_with_cot_reasons</th>\n",
       "      <th>qs_relevance</th>\n",
       "      <th>relevance_calls</th>\n",
       "      <th>groundedness_measure_with_cot_reasons_calls</th>\n",
       "      <th>qs_relevance_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LlamaIndex_App1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_22762836f0873b6288489f235c54e5bf</td>\n",
       "      <td>\"how any orders have been shipped?\"</td>\n",
       "      <td>\"This context does not mention anything about ...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_22762836f0873b62884...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-22T18:17:24.780966\", \"...</td>\n",
       "      <td>2023-12-22T18:17:31.556437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>[{'args': {'prompt': 'how any orders have been...</td>\n",
       "      <td>[{'args': {'source': ['WITH products AS (\\n   ...</td>\n",
       "      <td>[{'args': {'question': 'how any orders have be...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LlamaIndex_App1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_8ecd255e7ca1d761622b4e5e6c1efe72</td>\n",
       "      <td>\"how any orders have been shipped?\"</td>\n",
       "      <td>\"This context does not mention anything about ...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_8ecd255e7ca1d761622...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-22T18:17:38.947006\", \"...</td>\n",
       "      <td>2023-12-22T18:17:45.110991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[{'args': {'prompt': 'how any orders have been...</td>\n",
       "      <td>[{'args': {'source': ['WITH products AS (\\n   ...</td>\n",
       "      <td>[{'args': {'question': 'how any orders have be...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LlamaIndex_App1</td>\n",
       "      <td>{\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_be64e3d6ec56d48f87ecce7f72ec5e59</td>\n",
       "      <td>\"how any orders have been shipped?\"</td>\n",
       "      <td>\"This context does not mention anything about ...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_be64e3d6ec56d48f87e...</td>\n",
       "      <td>{\"n_requests\": 0, \"n_successful_requests\": 0, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-22T18:24:51.393517\", \"...</td>\n",
       "      <td>2023-12-22T18:24:58.720887</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'args': {'prompt': 'how any orders have been...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            app_id                                           app_json   \n",
       "0  LlamaIndex_App1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...  \\\n",
       "1  LlamaIndex_App1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "2  LlamaIndex_App1  {\"tru_class_info\": {\"name\": \"TruLlama\", \"modul...   \n",
       "\n",
       "                                                type   \n",
       "0  RetrieverQueryEngine(llama_index.query_engine....  \\\n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id   \n",
       "0  record_hash_22762836f0873b6288489f235c54e5bf  \\\n",
       "1  record_hash_8ecd255e7ca1d761622b4e5e6c1efe72   \n",
       "2  record_hash_be64e3d6ec56d48f87ecce7f72ec5e59   \n",
       "\n",
       "                                 input   \n",
       "0  \"how any orders have been shipped?\"  \\\n",
       "1  \"how any orders have been shipped?\"   \n",
       "2  \"how any orders have been shipped?\"   \n",
       "\n",
       "                                              output tags   \n",
       "0  \"This context does not mention anything about ...    -  \\\n",
       "1  \"This context does not mention anything about ...    -   \n",
       "2  \"This context does not mention anything about ...    -   \n",
       "\n",
       "                                         record_json   \n",
       "0  {\"record_id\": \"record_hash_22762836f0873b62884...  \\\n",
       "1  {\"record_id\": \"record_hash_8ecd255e7ca1d761622...   \n",
       "2  {\"record_id\": \"record_hash_be64e3d6ec56d48f87e...   \n",
       "\n",
       "                                           cost_json   \n",
       "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...  \\\n",
       "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "2  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "\n",
       "                                           perf_json   \n",
       "0  {\"start_time\": \"2023-12-22T18:17:24.780966\", \"...  \\\n",
       "1  {\"start_time\": \"2023-12-22T18:17:38.947006\", \"...   \n",
       "2  {\"start_time\": \"2023-12-22T18:24:51.393517\", \"...   \n",
       "\n",
       "                           ts  relevance   \n",
       "0  2023-12-22T18:17:31.556437        1.0  \\\n",
       "1  2023-12-22T18:17:45.110991        1.0   \n",
       "2  2023-12-22T18:24:58.720887        1.0   \n",
       "\n",
       "   groundedness_measure_with_cot_reasons  qs_relevance   \n",
       "0                                    0.0          0.22  \\\n",
       "1                                    0.0          0.20   \n",
       "2                                    NaN           NaN   \n",
       "\n",
       "                                     relevance_calls   \n",
       "0  [{'args': {'prompt': 'how any orders have been...  \\\n",
       "1  [{'args': {'prompt': 'how any orders have been...   \n",
       "2  [{'args': {'prompt': 'how any orders have been...   \n",
       "\n",
       "         groundedness_measure_with_cot_reasons_calls   \n",
       "0  [{'args': {'source': ['WITH products AS (\\n   ...  \\\n",
       "1  [{'args': {'source': ['WITH products AS (\\n   ...   \n",
       "2                                                NaN   \n",
       "\n",
       "                                  qs_relevance_calls  latency  total_tokens   \n",
       "0  [{'args': {'question': 'how any orders have be...        6             0  \\\n",
       "1  [{'args': {'question': 'how any orders have be...        6             0   \n",
       "2                                                NaN        7             0   \n",
       "\n",
       "   total_cost  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_records_and_feedback(app_ids=[])[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''query --metrics revenue_total,quantity_total --group-by order_date --where \"order_date >= DATE('now', '-7 days')\"'''\n",
    "'''query --metrics quantity_total,revenue_total --group-by product__product_name --limit 10 --order -quantity_total --where \"product__sell_start_date >= '2022-12-01'\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = subprocess.run(\"mf query --metrics revenue_total --group-by metric_time --start-time '2017-08-01' --end-time '2017-08-31' >\" +  \"../retrieval/data/output.txt\", \n",
    "                cwd=\"../semantics/\",\n",
    "                shell=True, text=True,\n",
    "                capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 32), match='--metrics revenue_total'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': ['revenue_total'],\n",
       " 'dimensions': ['revenue_total', 'metric_time'],\n",
       " 'start_time': '2017-08-01',\n",
       " 'end_time': '2017-08-31',\n",
       " 'where': 'product_order>6'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompose_metricflow_command(\"mf query --metrics revenue_total --group-by revenue_total,metric_time --start-time '2017-08-01' --end-time '2017-08-31' --where 'product_order>6' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metricflow import MetricFlowClient as mfc\n",
    "from warnings import f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decompose_metricflow_command(command_string):\n",
    "   \"\"\"Decomposes a MetricFlow command string into input parameters for a query function.\n",
    "\n",
    "   Args:\n",
    "       command_string: The MetricFlow command string to parse.\n",
    "\n",
    "   Returns:\n",
    "       A dictionary containing the extracted parameters:\n",
    "           metrics: A list of metrics.\n",
    "           dimensions: A list of dimensions.\n",
    "           limit: An integer for the limit (if present).\n",
    "           start_time: The start time (if present).\n",
    "           end_time: The end time (if present).\n",
    "           where: The WHERE clause (if present).\n",
    "           order: The ORDER clause (if present).\n",
    "   \"\"\"\n",
    "\n",
    "   params = {}\n",
    "\n",
    "   # Extract metrics\n",
    "   metrics_match = re.search(r\"--metrics (\\S+)\", command_string)\n",
    "   print(metrics_match)\n",
    "   if metrics_match:\n",
    "       params[\"metrics\"] = metrics_match.group(1).split(\",\")\n",
    "\n",
    "   # Extract dimensions\n",
    "   dimensions_match = re.search(r\"--group-by (\\S+)\", command_string)\n",
    "   if dimensions_match:\n",
    "       params[\"dimensions\"] = dimensions_match.group(1).split(\",\")\n",
    "\n",
    "   # Extract limit\n",
    "   limit_match = re.search(r\"--limit (\\d+)\", command_string)\n",
    "   if limit_match:\n",
    "       params[\"limit\"] = int(limit_match.group(1))\n",
    "\n",
    "   # Extract start_time\n",
    "   start_time_match = re.search(r\"--start-time '(\\S+)'\", command_string)\n",
    "   if start_time_match:\n",
    "       params[\"start_time\"] = start_time_match.group(1)\n",
    "\n",
    "   # Extract end_time\n",
    "   end_time_match = re.search(r\"--end-time '(\\S+)'\", command_string)\n",
    "   if end_time_match:\n",
    "       params[\"end_time\"] = end_time_match.group(1)\n",
    "\n",
    "   # Extract where clause\n",
    "   where_match = re.search(r\"--where '(.+)'\", command_string)\n",
    "   if where_match:\n",
    "       params[\"where\"] = where_match.group(1)\n",
    "\n",
    "   # Extract order clause\n",
    "   order_match = re.search(r\"--order (\\S+)\", command_string)\n",
    "   if order_match:\n",
    "       params[\"order\"] = order_match.group(1)\n",
    "\n",
    "   return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_query_input = \"query --metrics quantity_total --group-by sales_key__order_date --start-time '2017-08-01' --end-time '2017-08-30'\"\n",
    "output = subprocess.run(\"mf \"+ llm_query_input, \n",
    "                        cwd=\"./semantics/\",\n",
    "                        shell=True,\n",
    "                        text=True,\n",
    "                        capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "\n",
      "- Initiating query…\n",
      "\n",
      "\\ Initiating query…\n",
      "\n",
      "| Initiating query…\n",
      "\n",
      "/ Initiating query…\n",
      "v Success 🦄 - query completed after 3.49 seconds\n",
      "| sales_key__order_date__day   |   quantity_total |\n",
      "|:-----------------------------|-----------------:|\n",
      "| 2017-08-02                   |                8 |\n",
      "| 2017-08-03                   |                6 |\n",
      "| 2017-08-04                   |                9 |\n",
      "| 2017-08-05                   |                7 |\n",
      "| 2017-08-06                   |               12 |\n",
      "| 2017-08-07                   |               11 |\n",
      "| 2017-08-08                   |                8 |\n",
      "| 2017-08-09                   |                8 |\n",
      "| 2017-08-10                   |                4 |\n",
      "| 2017-08-12                   |                9 |\n",
      "| 2017-08-13                   |               12 |\n",
      "| 2017-08-14                   |               10 |\n",
      "| 2017-08-15                   |               12 |\n",
      "| 2017-08-16                   |                5 |\n",
      "| 2017-08-17                   |               10 |\n",
      "| 2017-08-18                   |                8 |\n",
      "| 2017-08-19                   |               11 |\n",
      "| 2017-08-20                   |               12 |\n",
      "| 2017-08-21                   |               10 |\n",
      "| 2017-08-22                   |               10 |\n",
      "| 2017-08-23                   |                5 |\n",
      "| 2017-08-24                   |               11 |\n",
      "| 2017-08-25                   |               11 |\n",
      "| 2017-08-26                   |               13 |\n",
      "| 2017-08-27                   |               11 |\n",
      "| 2017-08-28                   |                8 |\n",
      "| 2017-08-29                   |                6 |\n",
      "| 2017-08-30                   |                8 |\n",
      "| 2017-08-01                   |                9 |\n",
      "| 2017-08-11                   |                5 |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
