{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "import tiktoken\n",
    "import cohere\n",
    "import chromadb\n",
    "import tempfile\n",
    "import google.generativeai as genai\n",
    "from llama_index.llms import OpenAI, Gemini\n",
    "from llama_index.memory import ChatMemoryBuffer\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, PromptHelper, LLMPredictor, load_index_from_storage\n",
    "from llama_index.embeddings import OpenAIEmbedding, GeminiEmbedding\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer, LLMRerank, CohereRerank, LongContextReorder\n",
    "from llama_index import download_loader\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from flask import Flask, jsonify, flash, request, redirect, render_template, url_for, Response, stream_with_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = 'AIzaSyB4Aew8oVjBgPMZlskdhdmQs27DuyNBDAY'\n",
    "os.environ[\"GOOGLE_API_KEY\"]  = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    client_options={\"api_endpoint\": \"generativelanguage.googleapis.com\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMADB_HOST = \"localhost\"\n",
    "COHERE_RERANK_KEY = 'p8K3ASZaficAE1YlOh9dAY3x5Tkxa8sOmCRtJOtP'\n",
    "ALLOWED_EXTENSIONS = {'txt', 'htm', 'html', 'pdf', 'doc', 'docx', 'ppt', 'pptx', 'csv'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the chroma server, run the following in terminal:\n",
    "\n",
    "`chroma run --path ./src/vector_db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector_embedding(index_name, temp_dir):\n",
    "        \n",
    "    try:\n",
    "        # initialize client, setting path to save data\n",
    "        # db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "        print(\"Connecting to Chroma database...\")\n",
    "        db = chromadb.HttpClient(host=CHROMADB_HOST, port=8000)\n",
    "    except:\n",
    "        return {'statusCode': 400, 'status': 'Could not connect to chroma database'}\n",
    "\n",
    "    try:\n",
    "        # create collection\n",
    "        print(\"Creating vector embeddings......\")\n",
    "        print(\"Index name: \", index_name)\n",
    "        start_time = time.time()\n",
    "        chroma_collection = db.get_or_create_collection(\n",
    "            name=index_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"} # default: L2; used before: ip\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\"Error : : :\", e)\n",
    "        return {'statusCode': 400, 'status': 'A knowledge base with the same name already exists'}\n",
    "\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    # setup our storage (vector db)\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "\n",
    "    llm = Gemini(api_key=GOOGLE_API_KEY, model='models/gemini-pro', temperature=0.6)\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "    #     )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "        )\n",
    "\n",
    "    documents = SimpleDirectoryReader(input_dir=temp_dir).load_data()\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    # temp_dir.cleanup() # delete document temp dir\n",
    "\n",
    "    print(f\"Vector embeddings created in {time.time() - start_time} seconds.\")\n",
    "\n",
    "    response = {\n",
    "        'statusCode': 200,\n",
    "        'status': 'Chroma embedding complete',\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_vector_db(index_name):\n",
    "    \n",
    "    try:\n",
    "        # initialize client\n",
    "        db = chromadb.HttpClient(host=CHROMADB_HOST, port=8000)\n",
    "    except Exception as e:\n",
    "        print('<<< get_index_from_vector_db() >>> Could not connect to database!\\n', e)\n",
    "        return None, None\n",
    "    \n",
    "    # get collection and embedding size\n",
    "    try:\n",
    "        chroma_collection = db.get_collection(index_name)\n",
    "        doc_size = chroma_collection.count()\n",
    "        print('Computing knowledge base size...', doc_size)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "    start_time = time.time()\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Experimented settings for large docs versus small. Don't change except you have tested extensively!!!\n",
    "    # context_window=65000 if doc_size>200 else 16384\n",
    "    context_window=32000 if doc_size>300 else 2048\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    llm = Gemini(api_key=GOOGLE_API_KEY, model='models/gemini-pro', max_tokens=4096, temperature=0.3)\n",
    "    print_msg = \"Using Gemini Pro...\"\n",
    "    print(print_msg)\n",
    "\n",
    "    # node_parser = SimpleNodeParser.from_defaults(\n",
    "    #     text_splitter=TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "    #     )\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        context_window=context_window, \n",
    "        embed_model=embed_model,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    print('Retrieving knowledge base index from ChromaDB...')            \n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store, \n",
    "        storage_context=storage_context,\n",
    "        service_context=service_context\n",
    "    )\n",
    "\n",
    "    print(f'Index retrieved from ChromaDB in {time.time() - start_time} seconds.')\n",
    "    return index, doc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessor_args(doc_size):\n",
    "    if doc_size<500:\n",
    "        return None\n",
    "    \n",
    "    print('Optimising context information...')\n",
    "    \n",
    "    # fastest postprocessor\n",
    "    cohere_rerank = CohereRerank(api_key=COHERE_RERANK_KEY, top_n=20)\n",
    "\n",
    "    # slowest postprocessor using GPT-3.5, fairly fast using MockLLM\n",
    "    embed_model = GeminiEmbedding(api_key=GOOGLE_API_KEY)\n",
    "    service_context = ServiceContext.from_defaults(llm=None, embed_model=embed_model, chunk_size=256, chunk_overlap=20) # use llama_index default MockLLM (faster)\n",
    "    # service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\",), chunk_size=512, chunk_overlap=0)\n",
    "    rank_postprocessor = LLMRerank(\n",
    "        choice_batch_size=10, top_n=100,\n",
    "        service_context=service_context,\n",
    "        parse_choice_select_answer_fn=parse_choice_select_answer_fn\n",
    "    )\n",
    "    \n",
    "    # node postprocessors run in the specified order\n",
    "    node_postprocessors = [\n",
    "        rank_postprocessor,\n",
    "        cohere_rerank,\n",
    "    ]\n",
    "\n",
    "    return node_postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_choice_select_answer_fn(\n",
    "    answer: str, num_choices: int, raise_error: bool = False\n",
    "):\n",
    "    \"\"\"Default parse choice select answer function.\"\"\"\n",
    "    answer_lines = answer.split(\"\\n\")\n",
    "    # print(answer_lines)\n",
    "    answer_nums = []\n",
    "    answer_relevances = []\n",
    "    for answer_line in answer_lines:\n",
    "        line_tokens = answer_line.split(\",\")\n",
    "        if len(line_tokens) != 2:\n",
    "            if not raise_error:\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid answer line: {answer_line}. \"\n",
    "                    \"Answer line must be of the form: \"\n",
    "                    \"answer_num: <int>, answer_relevance: <float>\"\n",
    "                )\n",
    "        if len(line_tokens[0].split(\":\"))>1 and line_tokens[0].split(\":\")[1].strip().isdigit():\n",
    "            answer_num = int(line_tokens[0].split(\":\")[1].strip())\n",
    "            if answer_num > num_choices:\n",
    "                continue\n",
    "            answer_nums.append(answer_num)\n",
    "            answer_relevances.append(float(line_tokens[1].split(\":\")[1].strip()))\n",
    "    # print(answer_nums)\n",
    "    return answer_nums, answer_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_stream(query, index_name, chat_history):\n",
    "\n",
    "    index, doc_size = get_index_from_vector_db(index_name)\n",
    "    prompt_header = prompt_style()\n",
    "\n",
    "    if index is None:\n",
    "        response = \"I'm sorry I couldn't find any document in your knowledge base. Please add documents to your knowledge base and try again.\"\n",
    "        return response\n",
    "    else:\n",
    "        node_postprocessors = postprocessor_args(doc_size)\n",
    "        # similarity_top_k = 50 if doc_size>500 else 10\n",
    "        similarity_top_k = 300 if doc_size>500 else 15 if doc_size>200 else 20\n",
    "        chat_engine = index.as_chat_engine(chat_mode=\"context\", \n",
    "                                            memory=chat_history,\n",
    "                                            system_prompt=prompt_header, \n",
    "                                            similarity_top_k=similarity_top_k,\n",
    "                                            verbose=True, \n",
    "                                            # streaming=True,\n",
    "                                            function_call=\"query_engine_tool\",\n",
    "                                            node_postprocessors=node_postprocessors\n",
    "                                            )\n",
    "\n",
    "        message_body = f\"\"\"\\nUse the tool to answer:\\n{query}\\n\"\"\"\n",
    "        response = chat_engine.chat(message_body)\n",
    "        # print(get_formatted_sources(response) if response.source_nodes else None)\n",
    "        \n",
    "        if response is None:\n",
    "            print(\"Index retrieved but cannot stream response...\")\n",
    "            chat_response = \"I'm sorry I couldn't find an answer to the requested information in your knowledge base. Please rephrase your question and try again.\"\n",
    "            # for token in chat_response.split():\n",
    "            #     print(token, end=\" \")\n",
    "            #     yield f\"\"\"{token} \"\"\"\n",
    "            return chat_response\n",
    "        else:\n",
    "            print('Starting response stream...\\n...........................\\n...........................')\n",
    "            # return response.response\n",
    "            # for token in response.response_gen:\n",
    "            #     print(token, end=\"\")\n",
    "            #     yield f\"\"\"{token}\"\"\"\n",
    "            print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_formatted_sources(response, length=100, trim_text=True) -> str:\n",
    "    \"\"\"Get formatted sources text.\"\"\"\n",
    "    from llama_index.utils import truncate_text\n",
    "    texts = []\n",
    "    for source_node in response.source_nodes:\n",
    "        fmt_text_chunk = source_node.node.get_content()\n",
    "        if trim_text:\n",
    "            fmt_text_chunk = truncate_text(fmt_text_chunk, length)\n",
    "        # node_id = source_node.node.node_id or \"None\"\n",
    "        node_id = source_node.node.metadata['page_label'] or \"None\"\n",
    "        source_text = f\"> Source (Page no: {node_id}): {fmt_text_chunk}\"\n",
    "        texts.append(source_text)\n",
    "    return \"\\n\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_style(): \n",
    "\n",
    "    prompt_header = f\"\"\"Your name is Alpha. \n",
    "    You are a helpful and friendly Q&A bot, a highly intelligent system that \n",
    "    answers my questions based on the information I provide above each question. \n",
    "    Only use the information in the knowledge base I have provided, and if the information can not be found, decline to answer it. \n",
    "    \"\"\"\n",
    "\n",
    "    return prompt_header   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"Ebible\"\n",
    "index_name = project_name.lower() + '_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chroma database...\n",
      "Creating vector embeddings......\n",
      "Index name:  ebible_embeddings\n",
      "Vector embeddings created in 756.6599473953247 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'status': 'Chroma embedding complete'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_vector_embedding(index_name, './temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_thread(memory, reset=None):\n",
    "    if reset:\n",
    "        new_conversation_state = init_chat_history()\n",
    "        return new_conversation_state\n",
    "    return memory\n",
    "\n",
    "def init_chat_history():\n",
    "    new_conversation_state = ChatMemoryBuffer.from_defaults(token_limit=50000)\n",
    "    return new_conversation_state\n",
    "\n",
    "\n",
    "chat_history = init_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"State the verses where Demas is mentioned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing knowledge base size... 1607\n",
      "Using Gemini Pro...\n",
      "Retrieving knowledge base index from ChromaDB...\n",
      "Index retrieved from ChromaDB in 2.4284427165985107 seconds.\n",
      "Optimising context information...\n",
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Starting response stream...\n",
      "...........................\n",
      "...........................\n",
      "This context does not mention anything about Demas, so I cannot extract the requested data from the provided context.\n"
     ]
    }
   ],
   "source": [
    "answer_query_stream(query, index_name, chat_history)\n",
    "    # for token in response.response_gen:\n",
    "    # print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
